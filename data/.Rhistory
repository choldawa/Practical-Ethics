threshold,850)$val
}
# total default rate with baserates
default_rate_total = function(thresholds){
dr = purrr::pmap_dbl(data.frame(threshold = thresholds, m = race_means, s = race_sds),
default_rate)
sum(dr*race_rates)/sum(race_rates) # in case we dont have it sum to 1
}
# find constant thresholds
find.constant.thresholds = function(def.limit){
t = uniroot(function(t){
default_rate_total(rep(t, 4))-def.limit
},
lower=300,
upper=850)$root
rep(t, 4)
}
# find constant conditional probability
find.equal.thresholds = function(def.limit){
p = uniroot(function(p){
default_rate_total(qnorm(1-p, race_means, race_sds))-def.limit},
lower=0.001,
upper=0.999)$root
qnorm(1-p, race_means, race_sds)
}
# find threshold for interpolated p
thresholds_p1 = find.equal.thresholds(def.limit)
deltas = thresholds_p1 - mean(thresholds_p1)
find.interp.thresholds = function(p, def.limit){
mu = uniroot(function(mu){
thresholds = deltas*p + mu
default_rate_total(thresholds)-def.limit
}, lower=300,
upper=850)$root
mu+deltas*p
}
#Find approval percent
slider_start = -30
slider_end = slider_start+200
p_list = seq(slider_start, slider_end, by = 1)
out = matrix(ncol = length(find.interp.thresholds(0.5, def.limit)),
nrow = length(p_list))
for (p in p_list){
for(i in 1:length(race_means))
out[(p-(slider_start-1)),i] = round((1-pnorm(find.interp.thresholds(p/100, def.limit)[i], race_means[i], race_sds[i]))*100,0)
#out[p+1,i] = (1-pnorm(find.interp.thresholds(p/100, def.limit)[i], race_means[i], race_sds[i]))
}
df = data.frame(out)
df$mu = rowMeans(df[,1:4])
colnames(df) = c('asian','white','black','hispanic', 'mu')
df$type = "HIGH(-70,130)"
out.json = toJSON(df, pretty = T)
write(out.json, "./FICOTest.json")
l
l
l
l
#Find approval percent
slider_start = -30
slider_end = slider_start+200
p_list = seq(slider_start, slider_end, by = 1)
out = matrix(ncol = length(find.interp.thresholds(0.5, def.limit)),
nrow = length(p_list))
for (p in p_list){
for(i in 1:length(race_means))
out[(p-(slider_start-1)),i] = round((1-pnorm(find.interp.thresholds(p/100, def.limit)[i], race_means[i], race_sds[i]))*100,0)
#out[p+1,i] = (1-pnorm(find.interp.thresholds(p/100, def.limit)[i], race_means[i], race_sds[i]))
}
df = data.frame(out)
df$mu = rowMeans(df[,1:4])
colnames(df) = c('asian','white','black','hispanic', 'mu')
df$type = "HIGH(-30,130)"
out.json = toJSON(df, pretty = T)
write(out.json, "./FICOTest.json")
df$type = "HIGH(-30,170)"
out.json = toJSON(df, pretty = T)
write(out.json, "./FICOTest.json")
race_means = c('asian'=720, 'white'=640, 'black'=480, 'hispanic'=560) #med diff
mu_type = 'MEDIUM: A:750, W:700, B:450, H:500'
# pdf of fico score given the fico truncation.
dtnorm = function(x, m, s){
dnorm(x, m, s) / (pnorm(850, m, s) - pnorm(300, m, s))
}
# default-rate for fico
def_rate_fico = function(x){(1-1/(1+exp(-.016*(x-550))))}
# default rate for threshold, mu, sd
default_rate = function(threshold, m, s){
integrate(function(x){
dtnorm(x,m,s)*def_rate_fico(x)
},
threshold,850)$val
}
# total default rate with baserates
default_rate_total = function(thresholds){
dr = purrr::pmap_dbl(data.frame(threshold = thresholds, m = race_means, s = race_sds),
default_rate)
sum(dr*race_rates)/sum(race_rates) # in case we dont have it sum to 1
}
# find constant thresholds
find.constant.thresholds = function(def.limit){
t = uniroot(function(t){
default_rate_total(rep(t, 4))-def.limit
},
lower=300,
upper=850)$root
rep(t, 4)
}
# find constant conditional probability
find.equal.thresholds = function(def.limit){
p = uniroot(function(p){
default_rate_total(qnorm(1-p, race_means, race_sds))-def.limit},
lower=0.001,
upper=0.999)$root
qnorm(1-p, race_means, race_sds)
}
# find threshold for interpolated p
thresholds_p1 = find.equal.thresholds(def.limit)
deltas = thresholds_p1 - mean(thresholds_p1)
find.interp.thresholds = function(p, def.limit){
mu = uniroot(function(mu){
thresholds = deltas*p + mu
default_rate_total(thresholds)-def.limit
}, lower=300,
upper=850)$root
mu+deltas*p
}
#Find approval percent
slider_start = -40
slider_end = slider_start+200
p_list = seq(slider_start, slider_end, by = 1)
out = matrix(ncol = length(find.interp.thresholds(0.5, def.limit)),
nrow = length(p_list))
for (p in p_list){
for(i in 1:length(race_means))
out[(p-(slider_start-1)),i] = round((1-pnorm(find.interp.thresholds(p/100, def.limit)[i], race_means[i], race_sds[i]))*100,0)
#out[p+1,i] = (1-pnorm(find.interp.thresholds(p/100, def.limit)[i], race_means[i], race_sds[i]))
}
df = data.frame(out)
df$mu = rowMeans(df[,1:4])
colnames(df) = c('asian','white','black','hispanic', 'mu')
df$type = "HIGH(-40,160)"
out.json = toJSON(df, pretty = T)
write(out.json, "./FICOTest.json")
df$type = "MED(-40,160)"
out.json = toJSON(df, pretty = T)
write(out.json, "./FICOTest.json")
# means
race_means = c('asian'=660, 'white'=620, 'black'=540, 'hispanic'=580) #low diff
mu_type = 'LOW: A:641, W:636, B:528, H:583'
# pdf of fico score given the fico truncation.
dtnorm = function(x, m, s){
dnorm(x, m, s) / (pnorm(850, m, s) - pnorm(300, m, s))
}
# default-rate for fico
def_rate_fico = function(x){(1-1/(1+exp(-.016*(x-550))))}
# default rate for threshold, mu, sd
default_rate = function(threshold, m, s){
integrate(function(x){
dtnorm(x,m,s)*def_rate_fico(x)
},
threshold,850)$val
}
# total default rate with baserates
default_rate_total = function(thresholds){
dr = purrr::pmap_dbl(data.frame(threshold = thresholds, m = race_means, s = race_sds),
default_rate)
sum(dr*race_rates)/sum(race_rates) # in case we dont have it sum to 1
}
# find constant thresholds
find.constant.thresholds = function(def.limit){
t = uniroot(function(t){
default_rate_total(rep(t, 4))-def.limit
},
lower=300,
upper=850)$root
rep(t, 4)
}
# find constant conditional probability
find.equal.thresholds = function(def.limit){
p = uniroot(function(p){
default_rate_total(qnorm(1-p, race_means, race_sds))-def.limit},
lower=0.001,
upper=0.999)$root
qnorm(1-p, race_means, race_sds)
}
# find threshold for interpolated p
thresholds_p1 = find.equal.thresholds(def.limit)
deltas = thresholds_p1 - mean(thresholds_p1)
find.interp.thresholds = function(p, def.limit){
mu = uniroot(function(mu){
thresholds = deltas*p + mu
default_rate_total(thresholds)-def.limit
}, lower=300,
upper=850)$root
mu+deltas*p
}
#Find approval percent
slider_start = -50
slider_end = slider_start+200
p_list = seq(slider_start, slider_end, by = 1)
out = matrix(ncol = length(find.interp.thresholds(0.5, def.limit)),
nrow = length(p_list))
for (p in p_list){
for(i in 1:length(race_means))
out[(p-(slider_start-1)),i] = round((1-pnorm(find.interp.thresholds(p/100, def.limit)[i], race_means[i], race_sds[i]))*100,0)
#out[p+1,i] = (1-pnorm(find.interp.thresholds(p/100, def.limit)[i], race_means[i], race_sds[i]))
}
df = data.frame(out)
df$mu = rowMeans(df[,1:4])
colnames(df) = c('asian','white','black','hispanic', 'mu')
df$type = "LOW(-50,150)"
out.json = toJSON(df, pretty = T)
write(out.json, "./FICOTest.json")
library(jsonlite)
library(tidyverse)
#Read in and merge all json data files
setwd("./data")
file_list <- list.files()
for (file in file_list){
print(file)
# if the merged dataset doesn't exist, create it
if (!exists("df_full")){
json.data = fromJSON(file)
df_full = data.frame(json.data[["trials"]][["response"]],
json.data[["trials"]][["att"]],
json.data[["trials"]][["trialNumber"]],
json.data[["trials"]][["prompt"]],
json.data[["trials"]][["chosenData"]][["mu"]],
json.data[["trials"]][["chosenData"]][["type"]])
df_full$id = json.data[["client"]][["sid"]]
}
# if the merged dataset does exist, append to it
if (exists("df_full")){
json.data = fromJSON(file)
temp_dataset = data.frame(json.data[["trials"]][["response"]],
json.data[["trials"]][["att"]],
json.data[["trials"]][["trialNumber"]],
json.data[["trials"]][["prompt"]],
json.data[["trials"]][["chosenData"]][["mu"]],
json.data[["trials"]][["chosenData"]][["type"]])
temp_dataset$id = json.data[["client"]][["sid"]]
df_full = rbind(df_full, temp_dataset)
rm(temp_dataset)
}
}
setwd("~/Documents/Projects/Practical-Ethics")
#Read in and merge all json data files
setwd("./data")
file_list <- list.files()
for (file in file_list){
print(file)
# if the merged dataset doesn't exist, create it
if (!exists("df_full")){
json.data = fromJSON(file)
df_full = data.frame(json.data[["trials"]][["response"]],
json.data[["trials"]][["att"]],
json.data[["trials"]][["trialNumber"]],
json.data[["trials"]][["prompt"]],
json.data[["trials"]][["chosenData"]][["mu"]],
json.data[["trials"]][["chosenData"]][["type"]])
df_full$id = json.data[["client"]][["sid"]]
}
# if the merged dataset does exist, append to it
if (exists("df_full")){
json.data = fromJSON(file)
temp_dataset = data.frame(json.data[["trials"]][["response"]],
json.data[["trials"]][["att"]],
json.data[["trials"]][["trialNumber"]],
json.data[["trials"]][["prompt"]],
json.data[["trials"]][["chosenData"]][["mu"]],
json.data[["trials"]][["chosenData"]][["type"]])
temp_dataset$id = json.data[["client"]][["sid"]]
df_full = rbind(df_full, temp_dataset)
rm(temp_dataset)
}
}
# if the merged dataset does exist, append to it
if (exists("df_full")){
json.data = fromJSON(file)
temp_dataset = data.frame(json.data[["trials"]][["response"]],
json.data[["trials"]][["att"]],
json.data[["trials"]][["trialNumber"]],
json.data[["trials"]][["prompt"]],
json.data[["trials"]][["chosenData"]][["mu"]],
json.data[["trials"]][["chosenData"]][["type"]])
temp_dataset$id = json.data[["client"]][["sid"]]
df_full = rbind(df_full, temp_dataset)
rm(temp_dataset)
}
for (file in file_list){
print(file)
# if the merged dataset doesn't exist, create it
if (!exists("df_full")){
json.data = fromJSON(file)
df_full = data.frame(json.data[["trials"]][["response"]],
json.data[["trials"]][["att"]],
json.data[["trials"]][["trialNumber"]],
json.data[["trials"]][["prompt"]],
json.data[["trials"]][["chosenData"]][["mu"]],
json.data[["trials"]][["chosenData"]][["type"]])
df_full$id = json.data[["client"]][["sid"]]
}
# if the merged dataset does exist, append to it
if (exists("df_full")){
json.data = fromJSON(file)
temp_dataset = data.frame(json.data[["trials"]][["response"]],
json.data[["trials"]][["att"]],
json.data[["trials"]][["trialNumber"]],
json.data[["trials"]][["prompt"]],
json.data[["trials"]][["chosenData"]][["mu"]],
json.data[["trials"]][["chosenData"]][["type"]])
temp_dataset$id = json.data[["client"]][["sid"]]
df_full = rbind(df_full, temp_dataset)
rm(temp_dataset)
}
}
df = df_full
colnames(df) = c("response","att", "trialNumber", "prompt", "mu", "dataType", "id")
df$response = as.numeric(df$response)
#attention checks
df %>% select(att,response) %>%  filter(att == 1)
#dataframe manipulation (can be done on group df)
df$prompt.type <- ifelse(grepl("loan", df$prompt, ignore.case = T), "loan",
ifelse(grepl("newspaper", df$prompt, ignore.case = T), "newspaper",
ifelse(grepl("bail", df$prompt, ignore.case = T), "bail",
ifelse(grepl("job", df$prompt, ignore.case = T), "job",
ifelse(grepl("meals", df$prompt, ignore.case = T), "meals",
ifelse(grepl("respirator", df$prompt, ignore.case = T), "respirator","Other"))))))
##remove attention trials
df = df %>%
filter(att == 0)
#plots of response by datatype and prompt
library(ggpubr)
ggerrorplot(df, x = "prompt.type", y = "response",
desc_stat = "mean_se")
df %>% mutate(name = fct_relevel(dataType,
"LOW(-50,150)",
"MED(-40,160)",
"HIGH(-30,170)")) %>%
ggerrorplot(x = "prompt.type", y = "mu",
desc_stat = "mean_se") +
facet_grid(.~name)+
theme(axis.text.x = element_text(angle = 90))
ggerrorplot(df, x = "prompt.type", y = "mu",
desc_stat = "mean_se")
ggerrorplot(df, x = "prompt.type", y = "mu",
desc_stat = "mean_se")+
ggtitle("Mu Distribution by Prompt")
df %>% mutate(name = fct_relevel(dataType,
"LOW(-50,150)",
"MED(-40,160)",
"HIGH(-30,170)")) %>%
ggerrorplot(x = "prompt.type", y = "mu",
desc_stat = "mean_se") +
facet_grid(.~name)+
theme(axis.text.x = element_text(angle = 90))
df %>% mutate(name = fct_relevel(dataType,
"LOW(-50,150)",
"MED(-40,160)",
"HIGH(-30,170)")) %>%
ggerrorplot(x = "prompt.type", y = "mu",
desc_stat = "mean_se") +
facet_grid(.~name)+
theme(axis.text.x = element_text(angle = 90))+
ggtitle("Mu by Prompt and Tradeoff")
df %>% mutate(name = fct_relevel(dataType,
"LOW(-50,150)",
"MED(-40,160)",
"HIGH(-30,170)")) %>%
ggerrorplot(x = "prompt.type", y = "response",
desc_stat = "mean_se") +
facet_grid(.~name)+
theme(axis.text.x = element_text(angle = 90))+
ggtitle("Mu by Prompt and Tradeoff")
#Distribution of responses
ggplot(df, aes(response)) +
geom_histogram(bins = 15) +facet_grid(.~dataType)
##Create DF to compare correlation between trials
df_first_obs = df %>%
arrange(trialNumber) %>%
group_by(id, prompt.type, dataType) %>%
slice(1) %>%
select(id, dataType, prompt.type, mu, response)
df_second_obs = df %>% arrange(trialNumber) %>%
group_by(id, prompt.type, dataType) %>% slice(n()) %>%
select(id, dataType, prompt.type, mu, response)
df_grouped = left_join(df_first_obs, df_second_obs, by=c("id","dataType", "prompt.type")) %>% rowwise()
#create equation for correlation
corr_eqn = function(x,y, digits = 2) {
corr_coef <- round(cor(x, y), digits = digits)
paste("italic(r) == ", corr_coef)
}
labels = data.frame(x = 25, y = 55, label = corr_eqn(df_grouped$mu.x, df_grouped$mu.y))
ggplot(df_grouped, aes(x = mu.x, y = mu.y))+
geom_point()+
ggtitle("Correlation of Mu (repeated trials) within subject")+
xlab("Mu (first trial)") +
ylab("Mu (second trial)")+
geom_smooth(method = "lm")+
geom_text(data = labels, aes(x = x, y = y,
label = label), parse = TRUE)
glimpse(df_grouped)
head(df_grouped, 10)
ggplot(df_grouped, aes(x = response.x, y = response.y))+
geom_point()+
ggtitle("Correlation of Mu (repeated trials) within subject")+
xlab("Mu (first trial)") +
ylab("Mu (second trial)")+
geom_smooth(method = "lm")+
geom_text(data = labels, aes(x = x, y = y,
label = label), parse = TRUE)
labels = data.frame(x = 25, y = 55, label = corr_eqn(df_grouped$response.x, df_grouped$response.y))
ggplot(df_grouped, aes(x = response.x, y = response.y))+
geom_point()+
ggtitle("Correlation of Mu (repeated trials) within subject")+
xlab("Mu (first trial)") +
ylab("Mu (second trial)")+
geom_smooth(method = "lm")+
geom_text(data = labels, aes(x = x, y = y,
label = label), parse = TRUE)
ggplot(df_grouped, aes(x = response.x, y = response.y))+
geom_point()+
ggtitle("Correlation of Mu (repeated trials) within subject")+
xlab("Mu (first trial)") +
ylab("Mu (second trial)")+
geom_smooth(method = "lm")+
geom_text(data = labels, aes(label = label), parse = TRUE)
ggplot(df_grouped, aes(x = response.x, y = response.y))+
geom_point()+
ggtitle("Correlation of Mu (repeated trials) within subject")+
xlab("Mu (first trial)") +
ylab("Mu (second trial)")+
geom_smooth(method = "lm")+
geom_text(data = labels, aes(label = label), parse = TRUE)
ggplot(df_grouped, aes(x = response.x, y = response.y))+
geom_point()+
ggtitle("Correlation of Mu (repeated trials) within subject")+
xlab("Mu (first trial)") +
ylab("Mu (second trial)")+
geom_smooth(method = "lm")+
geom_text(data = labels, aes(label = label), parse = TRUE)
ggplot(df_grouped, aes(x = response.x, y = response.y))+
geom_point()+
ggtitle("Correlation of Mu (repeated trials) within subject")+
xlab("Mu (first trial)") +
ylab("Mu (second trial)")+
geom_smooth(method = "lm")+
geom_text(data = labels, aes(x = 150, y = 150, label = label), parse = TRUE)
ggplot(df_grouped, aes(x = response.x, y = response.y))+
geom_point()+
ggtitle("Correlation of Mu (repeated trials) within subject")+
xlab("Mu (first trial)") +
ylab("Mu (second trial)")+
geom_smooth(method = "lm")+
geom_text(data = labels, aes(x = 0, y = 150, label = label), parse = TRUE)
max(df_grouped(response.x))
max(df_grouped$response.x)
ggplot(df_grouped, aes(x = mu.x, y = mu.y))+
geom_point()+
ggtitle("Correlation of Mu (repeated trials) within subject")+
xlab("Mu (first trial)") +
ylab("Mu (second trial)")+
geom_smooth(method = "lm")+
geom_text(data = labels, aes(x = 0, y = 150, label = label), parse = TRUE)
labels = data.frame(x = 25, y = 55, label = corr_eqn(df_grouped$mu.x, df_grouped$mu.y))
ggplot(df_grouped, aes(x = mu.x, y = mu.y))+
geom_point()+
ggtitle("Correlation of Mu (repeated trials) within subject")+
xlab("Mu (first trial)") +
ylab("Mu (second trial)")+
geom_smooth(method = "lm")+
geom_text(data = labels, aes(x = 0, y = 150, label = label), parse = TRUE)
ggplot(df_grouped, aes(x = mu.x, y = mu.y))+
geom_point()+
ggtitle("Correlation of Mu (repeated trials) within subject")+
xlab("Mu (first trial)") +
ylab("Mu (second trial)")+
geom_smooth(method = "lm")+
geom_text(data = labels, aes(x = 25, y = 55, label = label), parse = TRUE)
ggerrorplot(df, x = "prompt.type", y = "mu",
desc_stat = "mean_se")+
ggtitle("Mu Distribution by Prompt")
df %>% mutate(name = fct_relevel(dataType,
"LOW(-50,150)",
"MED(-40,160)",
"HIGH(-30,170)")) %>%
ggerrorplot(x = "prompt.type", y = "response",
desc_stat = "mean_se") +
facet_grid(.~name)+
theme(axis.text.x = element_text(angle = 90))+
ggtitle("Mu by Prompt and Tradeoff")
df %>% mutate(name = fct_relevel(dataType,
"LOW(-50,150)",
"MED(-40,160)",
"HIGH(-30,170)")) %>%
ggerrorplot(x = "prompt.type", y = "mu",
desc_stat = "mean_se") +
facet_grid(.~name)+
theme(axis.text.x = element_text(angle = 90))+
ggtitle("Mu by Prompt and Tradeoff")
df %>% mutate(name = fct_relevel(dataType,
"LOW(-50,150)",
"MED(-40,160)",
"HIGH(-30,170)")) %>%
ggerrorplot(x = "prompt.type", y = "response",
desc_stat = "mean_se") +
facet_grid(.~name)+
theme(axis.text.x = element_text(angle = 90))+
ggtitle("Response by Prompt and Tradeoff")
unique(df$id)
nunique(df$id)
length(unique(df$id))
