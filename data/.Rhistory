# default rate for threshold, mu, sd
default_rate = function(threshold, m, s){
integrate(function(x){
dtnorm(x,m,s)*def_rate_fico(x)
},
threshold,850)$val
}
# total default rate with baserates
default_rate_total = function(thresholds){
dr = purrr::pmap_dbl(data.frame(threshold = thresholds, m = race_means, s = race_sds),
default_rate)
sum(dr*race_rates)/sum(race_rates) # in case we dont have it sum to 1
}
# find constant thresholds
find.constant.thresholds = function(def.limit){
t = uniroot(function(t){
default_rate_total(rep(t, 4))-def.limit
},
lower=300,
upper=850)$root
rep(t, 4)
}
# find constant conditional probability
find.equal.thresholds = function(def.limit){
p = uniroot(function(p){
default_rate_total(qnorm(1-p, race_means, race_sds))-def.limit},
lower=0.001,
upper=0.999)$root
qnorm(1-p, race_means, race_sds)
}
# find threshold for interpolated p
thresholds_p1 = find.equal.thresholds(def.limit)
deltas = thresholds_p1 - mean(thresholds_p1)
find.interp.thresholds = function(p, def.limit){
mu = uniroot(function(mu){
thresholds = deltas*p + mu
default_rate_total(thresholds)-def.limit
}, lower=300,
upper=850)$root
mu+deltas*p
}
#Find approval percent
slider_start = -30
slider_end = slider_start+170
p_list = seq(slider_start, slider_end, by = 1)
out = matrix(ncol = length(find.interp.thresholds(0.5, def.limit)),
nrow = length(p_list))
for (p in p_list){
for(i in 1:length(race_means))
out[(p-(slider_start-1)),i] = round((1-pnorm(find.interp.thresholds(p/100, def.limit)[i], race_means[i], race_sds[i]))*100,0)
#out[p+1,i] = (1-pnorm(find.interp.thresholds(p/100, def.limit)[i], race_means[i], race_sds[i]))
}
df = data.frame(out)
df$mu = rowMeans(df[,1:4])
colnames(df) = c('asian','white','black','hispanic', 'mu')
df$tradeoff = mu_type
out.json = toJSON(df, pretty = T)
write(out.json, "./FICOTest.json")
df$p = p_list
df = df %>% gather(key = 'race', value = 'FICO', 1:5)
df %>% filter(p %in% c(0,25,50,75,100), race == "mu") %>% select(FICO)
theme_set(theme_minimal())
ggplot(data = df, aes(x=p, y = FICO))+
geom_line(aes(color = race),size = 1)+
scale_color_manual(values=c("deepskyblue4", "firebrick2","lightseagreen",'black',"coral"))+
ggtitle(paste("Def rate = ", def.limit, "\nBase rates = ", mu_type ))+
ylab('Approval %')
# means
race_means = c('asian'=675, 'white'=625, 'black'=525, 'hispanic'=575) #low diff
mu_type = 'LOW: A:675, W:625, B:525, H:575 (SD:200)'
race_sds = c('asian'=200, 'white'=200, 'black'=200, 'hispanic'=200)
# pdf of fico score given the fico truncation.
dtnorm = function(x, m, s){
dnorm(x, m, s) / (pnorm(850, m, s) - pnorm(300, m, s))
}
# default-rate for fico
def_rate_fico = function(x){(1-1/(1+exp(-.016*(x-550))))}
# default rate for threshold, mu, sd
default_rate = function(threshold, m, s){
integrate(function(x){
dtnorm(x,m,s)*def_rate_fico(x)
},
threshold,850)$val
}
# total default rate with baserates
default_rate_total = function(thresholds){
dr = purrr::pmap_dbl(data.frame(threshold = thresholds, m = race_means, s = race_sds),
default_rate)
sum(dr*race_rates)/sum(race_rates) # in case we dont have it sum to 1
}
# find constant thresholds
find.constant.thresholds = function(def.limit){
t = uniroot(function(t){
default_rate_total(rep(t, 4))-def.limit
},
lower=300,
upper=850)$root
rep(t, 4)
}
# find constant conditional probability
find.equal.thresholds = function(def.limit){
p = uniroot(function(p){
default_rate_total(qnorm(1-p, race_means, race_sds))-def.limit},
lower=0.001,
upper=0.999)$root
qnorm(1-p, race_means, race_sds)
}
# find threshold for interpolated p
thresholds_p1 = find.equal.thresholds(def.limit)
deltas = thresholds_p1 - mean(thresholds_p1)
find.interp.thresholds = function(p, def.limit){
mu = uniroot(function(mu){
thresholds = deltas*p + mu
default_rate_total(thresholds)-def.limit
}, lower=300,
upper=850)$root
mu+deltas*p
}
#Find approval percent
slider_start = -30
slider_end = slider_start+170
p_list = seq(slider_start, slider_end, by = 1)
out = matrix(ncol = length(find.interp.thresholds(0.5, def.limit)),
nrow = length(p_list))
for (p in p_list){
for(i in 1:length(race_means))
out[(p-(slider_start-1)),i] = round((1-pnorm(find.interp.thresholds(p/100, def.limit)[i], race_means[i], race_sds[i]))*100,0)
#out[p+1,i] = (1-pnorm(find.interp.thresholds(p/100, def.limit)[i], race_means[i], race_sds[i]))
}
df = data.frame(out)
df$mu = rowMeans(df[,1:4])
colnames(df) = c('asian','white','black','hispanic', 'mu')
df$tradeoff = mu_type
out.json = toJSON(df, pretty = T)
write(out.json, "./FICOTest.json")
df$p = p_list
df = df %>% gather(key = 'race', value = 'FICO', 1:5)
df %>% filter(p %in% c(0,25,50,75,100), race == "mu") %>% select(FICO)
theme_set(theme_minimal())
ggplot(data = df, aes(x=p, y = FICO))+
geom_line(aes(color = race),size = 1)+
scale_color_manual(values=c("deepskyblue4", "firebrick2","lightseagreen",'black',"coral"))+
ggtitle(paste("Def rate = ", def.limit, "\nBase rates = ", mu_type ))+
ylab('Approval %')
race_means = c('asian'=750, 'white'=650, 'black'=450, 'hispanic'=550) #med diff
mu_type = 'MEDIUM: A:750, W:650, B:450, H:550 (SD:200)'
# pdf of fico score given the fico truncation.
dtnorm = function(x, m, s){
dnorm(x, m, s) / (pnorm(850, m, s) - pnorm(300, m, s))
}
# default-rate for fico
def_rate_fico = function(x){(1-1/(1+exp(-.016*(x-550))))}
# default rate for threshold, mu, sd
default_rate = function(threshold, m, s){
integrate(function(x){
dtnorm(x,m,s)*def_rate_fico(x)
},
threshold,850)$val
}
# total default rate with baserates
default_rate_total = function(thresholds){
dr = purrr::pmap_dbl(data.frame(threshold = thresholds, m = race_means, s = race_sds),
default_rate)
sum(dr*race_rates)/sum(race_rates) # in case we dont have it sum to 1
}
# find constant thresholds
find.constant.thresholds = function(def.limit){
t = uniroot(function(t){
default_rate_total(rep(t, 4))-def.limit
},
lower=300,
upper=850)$root
rep(t, 4)
}
# find constant conditional probability
find.equal.thresholds = function(def.limit){
p = uniroot(function(p){
default_rate_total(qnorm(1-p, race_means, race_sds))-def.limit},
lower=0.001,
upper=0.999)$root
qnorm(1-p, race_means, race_sds)
}
# find threshold for interpolated p
thresholds_p1 = find.equal.thresholds(def.limit)
deltas = thresholds_p1 - mean(thresholds_p1)
find.interp.thresholds = function(p, def.limit){
mu = uniroot(function(mu){
thresholds = deltas*p + mu
default_rate_total(thresholds)-def.limit
}, lower=300,
upper=850)$root
mu+deltas*p
}
#Find approval percent
slider_start = -30
slider_end = slider_start+170
p_list = seq(slider_start, slider_end, by = 1)
out = matrix(ncol = length(find.interp.thresholds(0.5, def.limit)),
nrow = length(p_list))
for (p in p_list){
for(i in 1:length(race_means))
out[(p-(slider_start-1)),i] = round((1-pnorm(find.interp.thresholds(p/100, def.limit)[i], race_means[i], race_sds[i]))*100,0)
#out[p+1,i] = (1-pnorm(find.interp.thresholds(p/100, def.limit)[i], race_means[i], race_sds[i]))
}
df = data.frame(out)
df$mu = rowMeans(df[,1:4])
colnames(df) = c('asian','white','black','hispanic', 'mu')
df$tradeoff = mu_type
out.json = toJSON(df, pretty = T)
write(out.json, "./FICOTest.json")
df$p = p_list
df = df %>% gather(key = 'race', value = 'FICO', 1:5)
df %>% filter(p %in% c(0,25,50,75,100), race == "mu") %>% select(FICO)
theme_set(theme_minimal())
ggplot(data = df, aes(x=p, y = FICO))+
geom_line(aes(color = race),size = 1)+
scale_color_manual(values=c("deepskyblue4", "firebrick2","lightseagreen",'black',"coral"))+
ggtitle(paste("Def rate = ", def.limit, "\nBase rates = ", mu_type ))+
ylab('Approval %')
out
mean(c(75,65,41,53)
)
race_means = c('g1'=800, 'g2'=633, 'g3'=567, 'g4'=400) #med diff
mu_type = 'MED: g1:800, g2:633, g3:567, g4:400 (SD:567)'
race_sds = c('g1'=200, 'g2'=200, 'g3'=200, 'g4'=200)
# pdf of fico score given the fico truncation.
dtnorm = function(x, m, s){
dnorm(x, m, s) / (pnorm(850, m, s) - pnorm(300, m, s))
}
# default-rate for fico
def_rate_fico = function(x){(1-1/(1+exp(-.016*(x-550))))}
# default rate for threshold, mu, sd
default_rate = function(threshold, m, s){
integrate(function(x){
dtnorm(x,m,s)*def_rate_fico(x)
},
threshold,850)$val
}
# total default rate with baserates
default_rate_total = function(thresholds){
dr = purrr::pmap_dbl(data.frame(threshold = thresholds, m = race_means, s = race_sds),
default_rate)
sum(dr*race_rates)/sum(race_rates) # in case we dont have it sum to 1
}
# find constant thresholds
find.constant.thresholds = function(def.limit){
t = uniroot(function(t){
default_rate_total(rep(t, 4))-def.limit
},
lower=300,
upper=850)$root
rep(t, 4)
}
# find constant conditional probability
find.equal.thresholds = function(def.limit){
p = uniroot(function(p){
default_rate_total(qnorm(1-p, race_means, race_sds))-def.limit},
lower=0.001,
upper=0.999)$root
qnorm(1-p, race_means, race_sds)
}
# find threshold for interpolated p
thresholds_p1 = find.equal.thresholds(def.limit)
deltas = thresholds_p1 - mean(thresholds_p1)
find.interp.thresholds = function(p, def.limit){
mu = uniroot(function(mu){
thresholds = deltas*p + mu
default_rate_total(thresholds)-def.limit
}, lower=300,
upper=850)$root
mu+deltas*p
}
#Find approval percent
slider_start = -30
slider_end = slider_start+170
p_list = seq(slider_start, slider_end, by = 1)
out = matrix(ncol = length(find.interp.thresholds(0.5, def.limit)),
nrow = length(p_list))
for (p in p_list){
for(i in 1:length(race_means))
out[(p-(slider_start-1)),i] = round((1-pnorm(find.interp.thresholds(p/100, def.limit)[i], race_means[i], race_sds[i]))*100,0)
#out[p+1,i] = (1-pnorm(find.interp.thresholds(p/100, def.limit)[i], race_means[i], race_sds[i]))
}
df = data.frame(out)
df$mu = rowMeans(df[,1:4])
colnames(df) = c('g1','g2','g3','g4', 'mu')
df$tradeoff = mu_type
df$p = p_list
out.json = toJSON(df, pretty = T)
write(out.json, "./FICOTest.json")
df = df %>% gather(key = 'race', value = 'FICO', 1:5)
df %>% filter(p %in% c(0,25,50,75,100), race == "mu") %>% select(FICO)
theme_set(theme_minimal())
ggplot(data = df, aes(x=p, y = FICO))+
geom_line(aes(color = race),size = 1)+
scale_color_manual(values=c("deepskyblue4", "firebrick2","lightseagreen","coral",'black'))+
ggtitle(paste("Def rate = ", def.limit, "\n", mu_type ))+
ylab('Approval %')
df %>% filter(p %in% c(0,25,50,75,100), race == "mu") %>% select(FICO)
df %>% filter(p %in% c(0,25,50,70,100), race == "mu") %>% select(FICO)
race_means = c('g1'=920, 'g2'=707, 'g3'=493, 'g4'=280) #high diff
mu_type = 'HIGH: g1:920, g2:707, g3:493, g4:280 (SD = 200)'
race_sds = c('g1'=200, 'g2'=200, 'g3'=200, 'g4'=200)
# pdf of fico score given the fico truncation.
dtnorm = function(x, m, s){
dnorm(x, m, s) / (pnorm(850, m, s) - pnorm(300, m, s))
}
# default-rate for fico
def_rate_fico = function(x){(1-1/(1+exp(-.016*(x-550))))}
# default rate for threshold, mu, sd
default_rate = function(threshold, m, s){
integrate(function(x){
dtnorm(x,m,s)*def_rate_fico(x)
},
threshold,850)$val
}
# total default rate with baserates
default_rate_total = function(thresholds){
dr = purrr::pmap_dbl(data.frame(threshold = thresholds, m = race_means, s = race_sds),
default_rate)
sum(dr*race_rates)/sum(race_rates) # in case we dont have it sum to 1
}
# find constant thresholds
find.constant.thresholds = function(def.limit){
t = uniroot(function(t){
default_rate_total(rep(t, 4))-def.limit
},
lower=300,
upper=850)$root
rep(t, 4)
}
# find constant conditional probability
find.equal.thresholds = function(def.limit){
p = uniroot(function(p){
default_rate_total(qnorm(1-p, race_means, race_sds))-def.limit},
lower=0.001,
upper=0.999)$root
qnorm(1-p, race_means, race_sds)
}
# find threshold for interpolated p
thresholds_p1 = find.equal.thresholds(def.limit)
deltas = thresholds_p1 - mean(thresholds_p1)
find.interp.thresholds = function(p, def.limit){
mu = uniroot(function(mu){
thresholds = deltas*p + mu
default_rate_total(thresholds)-def.limit
}, lower=300,
upper=850)$root
mu+deltas*p
}
#Find approval percent
slider_start = -30
slider_end = slider_start+170
p_list = seq(slider_start, slider_end, by = 1)
out = matrix(ncol = length(find.interp.thresholds(0.5, def.limit)),
nrow = length(p_list))
for (p in p_list){
for(i in 1:length(race_means))
out[(p-(slider_start-1)),i] = round((1-pnorm(find.interp.thresholds(p/100, def.limit)[i], race_means[i], race_sds[i]))*100,0)
#out[p+1,i] = (1-pnorm(find.interp.thresholds(p/100, def.limit)[i], race_means[i], race_sds[i]))
}
df = data.frame(out)
df$mu = rowMeans(df[,1:4])
colnames(df) = c('g1','g2','g3','g4', 'mu')
df$tradeoff = mu_type
df$p = p_list
out.json = toJSON(df, pretty = T)
write(out.json, "./FICOTest.json")
df = df %>% gather(key = 'race', value = 'FICO', 1:5)
df %>% filter(p %in% c(0,25,50,60,100), race == "mu") %>% select(FICO)
library(jsonlite)
library(tidyverse)
#json.data = fromJSON("./sona-31385.json")
#Read in and merge all json data files
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
setwd("./data")
file_list <- list.files()
for (file in file_list){
print(file)
# if the merged dataset doesn't exist, create it
if (!exists("df_full")){
json.data = fromJSON(file)
df_full = data.frame(json.data[["trials"]][["response"]],
json.data[["trials"]][["att"]],
json.data[["trials"]][["trialNumber"]],
json.data[["trials"]][["prompt"]],
json.data[["trials"]][["chosenData"]][["g1"]],
json.data[["trials"]][["chosenData"]][["g2"]],
json.data[["trials"]][["chosenData"]][["g3"]],
json.data[["trials"]][["chosenData"]][["g4"]],
json.data[["trials"]][["chosenData"]][["mu"]],
json.data[["trials"]][["chosenData"]][["tradeoff"]],
json.data[["trials"]][["chosenData"]][["p"]],
json.data[["trials"]][["jitter"]])
df_full$id = json.data[["client"]][["sid"]]
}
# if the merged dataset does exist, append to it
if (exists("df_full")){
json.data = fromJSON(file)
temp_dataset = data.frame(json.data[["trials"]][["response"]],
json.data[["trials"]][["att"]],
json.data[["trials"]][["trialNumber"]],
json.data[["trials"]][["prompt"]],
json.data[["trials"]][["chosenData"]][["g1"]],
json.data[["trials"]][["chosenData"]][["g2"]],
json.data[["trials"]][["chosenData"]][["g3"]],
json.data[["trials"]][["chosenData"]][["g4"]],
json.data[["trials"]][["chosenData"]][["mu"]],
json.data[["trials"]][["chosenData"]][["tradeoff"]],
json.data[["trials"]][["chosenData"]][["p"]],
json.data[["trials"]][["jitter"]])
temp_dataset$id = json.data[["client"]][["sid"]]
df_full = rbind(df_full, temp_dataset)
rm(temp_dataset)
}
}
df = df_full %>% distinct()
colnames(df) = c("response","att", "trialNumber", "prompt", "g1","g2","g3","g4", "mu", "tradeoff", "p", "jitter", "id")
df$response = as.numeric(as.character(df$response))
#variance of response
df = df %>%
mutate(var = pmap_dbl(list(g1,g2,g3,g4), ~ var(c(...))))
df$sd = sqrt(df$var)
#attention checks
df %>% select(att,response, id, trialNumber) %>%  filter(att == 1)
#json.data[[2]][["promptCheck1"]]+json.data[[2]][["promptCheck2"]]
#dataframe manipulation (can be done on group df)
df$prompt.type <- ifelse(grepl("loan", df$prompt, ignore.case = T), "loan",
ifelse(grepl("newspaper", df$prompt, ignore.case = T), "newspaper",
ifelse(grepl("bail", df$prompt, ignore.case = T), "bail",
ifelse(grepl("job", df$prompt, ignore.case = T), "job",
ifelse(grepl("meals", df$prompt, ignore.case = T), "meals",
ifelse(grepl("respirator", df$prompt, ignore.case = T), "respirator","Other"))))))
df$tradeoff <- ifelse(grepl("LOW", df$tradeoff, ignore.case = T), "low",
ifelse(grepl("MED", df$tradeoff, ignore.case = T), "med","high"))
df$respJitter = df$response+df$jitter
#df$latentEquality = df$response+df$jitter-50
##remove attention trials
df = df %>%
filter(att == 0)
#plots of response by datatype and prompt
library(ggpubr)
df %>% mutate(name = fct_relevel(tradeoff,
"low",
"med",
"high")) %>%
ggerrorplot(x = "name", y = "p",
desc_stat = "mean_se")+
ggtitle("latentEquality Distribution by Tradeoff")
df %>% mutate(name = fct_relevel(prompt.type,
"bail",
"job",
"newspaper",
"meals",
"loan",
"respirator")) %>%
ggerrorplot(x = "name", y = "p",
desc_stat = "mean_se")+
ggtitle("latentEquality Distribution by Prompt")
df %>% mutate(tradeoff = fct_relevel(tradeoff,
"low",
"med",
"high"),
prompt = fct_relevel(prompt.type,
"bail",
"job",
"newspaper",
"meals",
"loan",
"respirator")) %>%
ggerrorplot(x = "prompt", y = "p",
desc_stat = "mean_se") +
facet_grid(.~tradeoff)+
theme(axis.text.x = element_text(angle = 90))+
ggtitle("latentEquality by Prompt and Tradeoff")
#Distribution of responses
ggplot(df, aes(p)) +
geom_histogram(bins = 15) +facet_grid(.~tradeoff)
ggplot(df, aes(x = trialNumber, y = p))+
geom_point()+
facet_grid(.~tradeoff)+
geom_smooth(method = "lm")
#Mu vs variance
ggplot(df, aes(x = mu, y = sd, color = tradeoff))+
geom_point()+
facet_grid(.~tradeoff)+
geom_smooth()
lin = lm(data = df, p ~ tradeoff+prompt.type+jitter)
summary(lin)
##Create DF to compare correlation between trials
df_first_obs = df %>%
arrange(trialNumber) %>%
group_by(id, prompt.type, tradeoff) %>%
slice(1) %>%
select(id, tradeoff, prompt.type, mu, response, p)
df_second_obs = df %>% arrange(trialNumber) %>%
group_by(id, prompt.type, tradeoff) %>% slice(n()) %>%
select(id, tradeoff, prompt.type, mu, response, p)
df_grouped = left_join(df_first_obs, df_second_obs, by=c("id","tradeoff", "prompt.type")) %>% rowwise()
#create equation for correlation
corr_eqn = function(x,y, digits = 2) {
corr_coef <- round(cor(x, y), digits = digits)
paste("italic(r) == ", corr_coef)
}
#labels = data.frame(x = 25, y = 55, label = corr_eqn(df_grouped$mu.x, df_grouped$mu.y))
ggplot(df_grouped, aes(x = p.x, y = p.y))+
geom_point()+
facet_grid(.~tradeoff)+
ggtitle("Correlation of LatentEquality (repeated trials) within subject")+
xlab("Response (first trial)") +
ylab("Reponse (second trial)") +
geom_smooth(method = "lm")
